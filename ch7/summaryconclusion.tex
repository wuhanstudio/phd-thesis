\chapter{Conclusion and Future Work}
\label{chpt:conclusion}

This thesis provides insights into the research question: Have adversarial attacks become a practical threat against deep learning applications? This chapter summarizes our contributions and addresses limitations that can be improved in future research.

\section{Thesis Summary}

The existence of adversarial examples sparked widespread interest and discussion in deep learning research. Small perturbations that are unperceivable by human eyes can result in significant shifts in model predictions, suggesting a fundamentally different approach to task comprehension compared to human cognition. Chapters \ref{chpt:driving}-\ref{chpt:classification} presented novel adversarial attacks, including real-time online white-box attacks, Human-in-the-Middle hardware attacks, and distributed black-box attacks. 

% Furthermore, Chapter 6 introduced how we used interpretable machine learning to understand model predictions for healthcare applications.

Adversarial attacks against classifiers have been well explored in academic research, while attacks against regression models are not as well understood. Chapter \ref{chpt:driving} proposed two real-time online attacks against the end-to-end driving regression model. These attacks achieved real-time performance using two simple yet effective adversarial loss functions and assumed no access to the ground truth. As a result, it was shown that not just classification models, but also regression models are vulnerable to adversarial attacks.

Chapter \ref{chpt:detection} introduced real-time online attacks against object detection models, for a combination of regression and classification tasks. In addition to adversarial filters that apply unperceivable perturbations to the entire image and adversarial patches that apply conspicuous perturbations to a part of the image, a new method, the adversarial overlay, was proposed to combine the strengths of adversarial filters and patches. However, all aforementioned adversarial attacks require access to the operating system to inject the perturbation, limiting the practicality of the methods. Thus, we proposed the Human-in-the-Middle hardware attack that could bypass the operating system by injecting the perturbation into the communication channel.

Chapter \ref{chpt:tracking} further tested adversarial attacks against \acrfull{tbd} frameworks, which consist of deep neural networks and traditional algorithms such as the Kalman Filter and Hungarian Algorithm. Our experimental results revealed that both the image-specific and image-agnostic white-box attacks introduced in Chapter \ref{chpt:detection} can effectively compromise the tracking models.

Following our findings on real-time online white-box attacks, our focus transitioned to black-box attacks in Chapter \ref{chpt:classification}. Although existing research has demonstrated the feasibility of query-efficient black-box attacks that do not require access to the model structure and weights, we revealed several common mistakes that exaggerated the effectiveness of attacks. For example, we shouldn't assume access to model input shape and pre-processing methods when attacking image classification cloud APIs. In addition, prior research has primarily focused on increasing the success rate and reducing the number of queries, while we exploited the load balancing to enable distributed black-box attacks that could reduce the attack query time.

In conclusion, white-box attacks exploit gradients to achieve real-time online attacks against classification and regression models. Adversarial perturbations can be generalized to other models because of the transferability of deep learning models and can be injected into the communication channel without requiring access to the operating system, putting embedded systems at risk. Black-box attacks, on the contrary, though slower due to the query process, can target machine learning cloud services without access to gradients. However, the effectiveness of existing query-efficient black-box attacks is overrated due to assumptions about access to input shapes and preprocessing methods. The existence of adversarial examples highlights the fundamental differences between the cognition of deep learning models and human cognition.
\section{Limitations \& Future Research}

Generating adversarial attacks is an optimization process that focuses on optimizing adversarial input rather than tuning the model parameters. Our attacks can be generalized to detection models that represent bounding boxes similar to YOLO. In addition, the distributed attack applies to all cloud APIs that use load balancing.

However, the fast-paced development of new attacks benefits from advances in optimization methods, resembling the ongoing cat-and-mouse game seen in cybersecurity between red and blue teams. As stronger attacks emerge, they prompt the development of new defense strategies, which in turn lead to the creation of even stronger attacks. This section explores future research directions that are not discussed in the thesis.

\subsection{Robust Physical Patch in Simulated Environments}

Physical patches are susceptible to variations in angle and distance due to the constraints of the non-differentiable rendering process. Adversarial patches printed on paper may differ from optimized digital patches due to shearing (viewing from different angles) and lighting conditions. If we can compute the gradient of adversarial loss functions with respect to texture and lighting, the optimized adversarial patch can become more resilient to environmental changes. 

On the other hand, as rendering techniques advance, more photorealistic game engines and renderers are being developed. In the context of autonomous driving, it is increasingly common to test safety-critical edge cases in simulation. Testing physical attacks in simulators offers greater efficiency compared to the physical world, as it allows for easy variation of weather and lighting conditions. Moreover, testing physical patches in simulators helps avoid wasting significant amounts of printing materials. As a result, leveraging differentiable renderers such as Pytorch 3D \citep{ravi2020pytorch3d}, Kaolin \citep{KaolinLibrary}, and Mitsuba 2 \citep{nimier2019mitsuba} can lead to more robust adversarial patches in a simulated environment. Researchers can then use photo-realistic game engines to effectively transfer the patch to the physical world.

\subsection{Multi-Agent Reinforcement Learning (MARL)}

Reinforcement learning learns from trial and error and can be trained in a virtual environment before being deployed to the physical world. Advances in physics engines and realistic renderers enable complex robot simulations and model training in simulators. However, this thesis did not investigate the impact of adversarial attacks involving multiple agents interacting with each other.

As expected, reinforcement learning models are vulnerable to adversarial attacks when perturbations are added to agents' observations \citep{chen2019adversarial}. Recent research also reveals that adversarial policies acting abnormally in a shared environment can induce extremely unlikely activations. For example, in a scenario where a red agent aims to prevent a blue agent from crossing a line, the red agent takes the unexpected action of intentionally falling to the ground, causing the blue agent to fall as well, thus allowing the red agent to win the game \citep{gleave2021adversarial}. Although the falling-to-ground action of the red agent resembles a random policy, this abnormal action effectively perturbs the decision-making of the blue agent. These situations should be thoroughly tested and theoretically analyzed to understand adversarial attacks in an interactive environment.

\subsection{Scientific Machine Learning (SciML)}

Unlike deep neural networks, traditional numerical simulation methods that use finite difference, finite element methods to solve Ordinary Differential Equations (ODE) and Partial Differential Equations (PDE) follow the underlying physical laws and are less vulnerable to adversarial attacks. Scientific Machine Learning (SciML) is a growing field that combines the strengths of traditional numerical methods and machine learning to address some of the biggest challenges in science. This raises the question: Will incorporating deep neural networks into these methods increase their vulnerability to adversarial attacks? 

% This thesis does not explore adversarial attacks in applications beyond computer vision.

On the other hand, existing research on enhancing the robustness of deep learning models can benefit from scientific knowledge. For example, residual networks reduce error by creating skip connections that feed the input and output of the current layer into the next layer, which is identical to the Euler method \citep{chen2018neural}. However, the Euler method, known for its simplicity, is considered a poor solver due to the accumulation of errors at each step, and these insights can guide improvements in residual layers. By understanding and applying scientific concepts, we can better interpret the underlying mechanisms of deep neural network architectures and enhance their performance and robustness.

\subsection{Interpretable Machine Learning for Healthcare}

Although deep neural networks are susceptible to adversarial attacks, interpretation methods can help verify whether the model's decision-making criteria are consistent with medical knowledge. For healthcare applications, interpretation methods can be employed to verify the decision-making criteria of machine learning models and ensure that model predictions align with medical knowledge.

On the other hand, deep learning models that achieve high accuracy provide fewer interpretations due to the trade-off between accuracy and interpretability. In healthcare applications, this poses a challenge as both high accuracy and interpretability are essential; relying on black-box models that medical practitioners cannot fully understand may pose risks to patients. In a separate research paper, we study the interpretability of machine learning models for medical applications and combine deep neural networks with human knowledge to understand how black-box models make diagnoses during the COVID-19 pandemic \citep{han2021interpret}. The interpretation results revealed that the three most indicative biomarkers identified by the models aligned with other medical experimental results.

% Having explored real-time online white-box attacks and time-efficient black-box attacks, we moved towards interpretation methods in Chapter 7. We experimented with six interpretation methods (PFI, PDP, ALE, ICE, LIME, SHAP) and two evaluation metrics (faithfulness and monotonicity) to interpret black-box machine learning models. The interpretation results revealed that the three most indicative biomarkers identified by the models aligned with other medical experimental results.

% \vspace{0.5cm}

% \textbf{How can we embrace deep learning models safely?}

% How can we interpret and safely utilize black-box models in medical contexts? How can we interpret and safely embrace black-box models, especially for medical applications?

% It is risky to put one's life in the hands of models that medical researchers do not fully understand. 
