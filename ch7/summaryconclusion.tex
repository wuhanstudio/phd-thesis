\chapter{Conclusion and Future Work}
\label{chpt:conclusion}

This thesis provides insights into the research question: Have adversarial attacks become a practical threat against deep learning applications? This chapter summarizes our contributions and addresses limitations that can be improved in future research.

\section{Thesis Summary}

The existence of adversarial examples sparked widespread interest and discussion in deep learning research. Small perturbations that are unperceivable by human eyes can result in significant shifts in model predictions, suggesting a fundamentally different approach to task comprehension compared to human cognition. Chapters 2-5 presented novel adversarial attacks, including real-time online white-box attacks, Human-in-the-Middle hardware attacks, and distributed black-box attacks. Furthermore, Chapter 6 introduced how we used interpretable machine learning to understand model predictions for healthcare applications.

Adversarial attacks against classifiers have been well-explored in academic research, while attacks against regression models are not as well understood. Chapter \ref{chpt:driving} proposed two real-time online attacks against the end-to-end driving regression model. These attacks achieved real-time performance using two simple yet effective adversarial loss functions and assumed no access to the ground truth. As a result, it was shown that not just classification models, but also regression models are vulnerable to adversarial attacks.

Chapter 3 introduced real-time online attacks against object detection models, for a combination of regression and classification tasks. Besides adversarial filters that apply unperceivable perturbations to the entire image and adversarial patches that apply conspicuous perturbations to a part of the image, a new method, the adversarial overlay, was proposed to combine the strengths of adversarial filters and patches. However, all aforementioned adversarial attacks require access to the operating system to inject the perturbation, limiting the practicality of the methods. Thus, we proposed the Human-in-the-Middle hardware attack that could bypass the operating system by injecting the perturbation into the communication channel.

Chapter \ref{chpt:tracking} further tested adversarial attacks against \acrfull{tbd} frameworks, which consist of deep neural networks and traditional algorithms like the Kalman Filter and Hungarian Algorithm. Our experimental results revealed that both image-specific and image-agnostic white-box attacks introduced in Chapter \ref{chpt:detection} can effectively compromise tracking models.

Following our findings on real-time online white-box attacks, our focus transitioned to black-box attacks in Chapter \ref{chpt:classification}. Although existing research has demonstrated the feasibility of query-efficient black-box attacks that do not require access to model structure and weights, we revealed several common mistakes that exaggerated the effectiveness of attacks. For example, we shouldn't assume access to model input shape and pre-processing methods when attacking image classification cloud APIs. Besides,  prior research has primarily focused on increasing the success rate and reducing the number of queries, while we exploited the load balancing to enable distributed black-box attacks that could reduce the attack query time.

Having explored real-time online white-box attacks and time-efficient black-box attacks, we moved towards interpretation methods in Chapter \ref{chpt:defence}. We experimented with six interpretation methods (PFI, PDP, ALE, ICE, LIME, SHAP) and two evaluation metrics (faithfulness and monotonicity) to interpret black-box machine learning models. The interpretation results revealed that the three most indicative biomarkers identified by the models aligned with other medical experimental results. Although deep neural networks are susceptible to adversarial attacks, interpretation methods can help verify if the model's decision-making criteria are consistent with medical knowledge.

In conclusion, white-box attacks exploit gradients to achieve real-time online attacks against classification and regression models. Adversarial perturbations can be generalized to other models due to the transferability of deep learning models and can be injected into the communication channel without requiring access to the operating system, putting embedded systems at risk. Black-box attacks, in contrast, though slower due to the query process, can target machine-learning cloud services without access to gradients. However, the effectiveness of existing query-efficient black-box attacks is overrated due to assumptions about access to input shapes and pre-processing methods. The existence of adversarial examples highlights the fundamental differences between the cognition of deep learning models and human cognition. For healthcare applications, interpretation methods can be employed to verify the decision-making criteria of machine learning models and ensure that model predictions align with medical knowledge.

\section{Limitations \& Future Research}

Generating adversarial attacks is an optimization process that focuses on optimizing the adversarial inputs rather than tuning model parameters. The fast-paced development of new attacks benefits from advances in optimization methods, resembling the ongoing cat-and-mouse game seen in cybersecurity between red and blue teams. As stronger attacks emerge, they prompt the development of new defense strategies, which in turn lead to the creation of even stronger attacks. This section explores future research directions that leverage new technologies not discussed in the thesis.

\subsection{Robust Physical Patch in Simulated Environments}

Physical patches are susceptible to variations in angle and distance due to the constraints of the non-differentiable rendering process. Adversarial patches printed on paper may differ from optimized digital patches due to shearing (viewing from different angles) and lighting conditions. If we can compute the gradient of adversarial loss functions with respect to texture and lighting, the optimized adversarial patch can become more resilient to environmental changes. 

On the other hand, as rendering techniques advance, more photo-realistic game engines and renderers are being developed. In the context of autonomous driving, it is increasingly common to test safety-critical edge cases in simulation. Testing physical attacks in simulators offers greater efficiency compared to the physical world, as it allows for easy variation of weather and lighting conditions. Moreover, testing physical patches in simulators helps avoid wasting significant amounts of printing materials. As a result, leveraging differentiable renderers such as Pytorch 3D \citep{ravi2020pytorch3d}, Kaolin \citep{KaolinLibrary}, and Mitsuba 2 \citep{nimier2019mitsuba} can lead to more robust adversarial patches in a simulated environment. Researchers can then use photo-realistic game engines to effectively transfer the patch to the physical world.

\subsection{Multi-Agent Reinforcement Learning (MARL)}

Reinforcement learning learns from trial and error and can be trained and verified in a virtual environment before deployed to the physical world. The development of physics engines and realistic renderers makes it feasible to simulate complex robots and train reinforcement learning models in simulators. However, this thesis did not investigate the impact of adversarial attacks involving multiple agents interacting with one another.

As expected, reinforcement learning models are vulnerable to adversarial attacks when perturbations are added to agents' observations \citep{chen2019adversarial}. Recent research also reveals that adversarial policies acting abnormally in a shared environment can induce extremely unlikely activations. For example, in a scenario where a red agent aims to prevent a blue agent from crossing a line, the red agent takes an unexpected action of intentionally falling to the ground, causing the blue agent to fall as well, thereby enabling the red agent to win the game \citep{gleave2021adversarial}. Although the red agent's falling-to-ground action resembles a random policy, this abnormal action effectively perturbs the blue agent's decision-making. These situations should be thoroughly tested and theoretically analyzed to understand adversarial attacks in an interactive environment.

\subsection{Scientific Machine Learning (SciML)}

Unlike deep neural networks, traditional numerical simulation methods that use finite difference, finite element methods to solve Ordinary Differential Equations (ODE) and Partial Differential Equations (PDE) follow the underlying physical laws and are less vulnerable to adversarial attacks. Scientific Machine Learning (SciML) is a growing field that combines the strengths of traditional numerical methods and machine learning to address some of the biggest challenges in science. This raises the question: Will incorporating deep neural networks into these methods increase their vulnerability to adversarial attacks? 

% This thesis does not explore adversarial attacks in applications beyond computer vision.

On the other hand, existing research on enhancing the robustness of deep learning models can benefit from scientific knowledge. For example, residual networks reduce error by creating skip connections that feed the input and output of the current layer into the next layer, which is identical to the Euler method \citep{chen2018neural}. However, the Euler method, known for its simplicity, is considered a poor solver due to error accumulation at each step, and these insights can guide improvements in residual layers. By understanding and applying scientific concepts, we can better interpret the underlying mechanisms of deep neural network architectures and enhance their performance and robustness.
