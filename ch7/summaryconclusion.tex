\chapter{Conclusion and Future Work}

In conclusion, this research intends to achieve practical adversarial attacks against deep learning models. We have made some progress in distributed black-box attacks (\textbf{Adversarial Classification}) and real-time white-box attacks (\textbf{Adversarial Driving}, \textbf{Adversarial Detection}, and the \textbf{Man-in-the-Middle hardware attack}).

\section{Thesis Summary}

In the last year, we'll try to devise adversarial attacks against object tracking and investigate the effects of hardware acceleration on our attacks. Eventually, we hope we can understand various models through interpretation, and thus deploying deep learning models more securely in robotics.

\clearpage

\section{Future Research}

We plan to generate adversarial patches against object detection models in a simulated environment by changing the material or texture of an object, which is a special kind of physical patch.
   
\subsection{Adversarial Patch}

For autonomous driving, it is more popular to test safety-critical edge cases in simulation. Testing physical attacks in simulators is more efficient than in the physical world since we can easily vary weather and lighting conditions. In simulators, adversarial patches can be applied by changing the material or texture of an object, but prior research only applies patches by modifying pixel values \cite{mathov2021enhancing} \cite{nesti2022evaluating} \cite{rossolini2022realworld}.

Besides, testing physical patches could waste a lot of printing materials.

In summary, most prior research tests physical attacks in the real world or applies adversarial patches by modifying pixel values in simulators.

\subsection{Reinforcement Learning}

Multi-agent Reinforcement Learning
