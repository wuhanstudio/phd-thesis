\chapter*{Abstract}

Advances in deep neural networks have opened a new era of robotics, intelligent robots. Compared with traditional robots that perform repetitive tasks based on manual control or predefined rules, intelligent robots possess a more comprehensive perception of environments and make more sophisticated decisions for various tasks.
 
However, it is no more a secret that deep learning models are vulnerable to adversarial attacks. Recent research unveils that deep neural networks are vulnerable to adversarial attacks. Image Classification, Object Detection, Object Tracking, Medical Applications.

This thesis aims to address two critical questions: Firstly, can we develop practical adversarial attacks against deep learning models? Secondly, how can we safely embrace the benefit of deep learning via interpretation methods?

To answer the question, Chapter \ref{chpt:intro} provides an introduction to various adversarial attacks on deep learning models, encompassing white-box attacks and black-box attacks. To achieve practical adversarial attacks, Chapter \ref{chpt:driving} presents real-time white-box attacks against end-to-end driving systems. Additionally, Chapter \ref{chpt:classification} explores distributed black-box attacks that leverage load balancing to accelerate the attack process. Chapter \ref{chpt:detection} aims to target object detection models, which play a crucial role in modular autonomous driving systems. Furthermore, Chapter \ref{chpt:tracking} focuses on attacking object tracking, which represents a more dynamic system. Given that testing safety-critical edge cases in simulators is more efficient in autonomous driving. Finally, in Chapter \ref{chpt:defence}, we delve into interpreting deep learning models and devising defense mechanisms against adversarial attacks.

%Chapter \ref{chpt:patch} introduces physical patches in the Carla Simulator. 
