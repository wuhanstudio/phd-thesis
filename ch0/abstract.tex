\chapter*{Abstract}
It is no more a secret that deep learning models are vulnerable to adversarial attacks. This research aims to address two critical questions: Firstly, can we develop practical adversarial attacks against deep learning models? Secondly, how can we defence against these attacks?

This thesis

Chapter \ref{chpt:intro} provides an introduction to various adversarial attacks on deep learning models, encompassing white-box attacks and black-box attacks.

To achieve practical adversarial attacks, Chapter \ref{chpt:driving} presents real-time white-box attacks against end-to-end driving systems. Additionally, Chapter \ref{chpt:classification} explores distributed black-box attacks that leverage load balancing to accelerate the attack process. Chapter \ref{chpt:detection} aims to target object detection models, which play a crucial role in modular autonomous driving systems. Furthermore, Chapter \ref{chpt:tracking} focuses on attacking object tracking, which represents a more dynamic system. Given that testing safety-critical edge cases in simulators is more efficient in autonomous driving.
%Chapter \ref{chpt:patch} introduces physical patches in the Carla Simulator.

Finally, in Chapter \ref{chpt:defence}, we delve into interpreting deep learning models and devising defense mechanisms against adversarial attacks.
