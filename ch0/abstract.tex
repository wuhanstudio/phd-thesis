\chapter*{Abstract}

Advances in deep neural networks have opened a new era of robotics, intelligent robots. Compared with traditional robots that perform repetitive tasks based on manual control or predefined rules, intelligent robots possess a more comprehensive perception of environments and make more sophisticated decisions for various tasks.
 
However, it is no more a secret that deep learning models are vulnerable to adversarial attacks. Recent research unveils that deep neural networks can be fooled by adding human unperceivable perturbations to the input data, posing threats against robotic applications that rely on deep neural networks to achieve image classification, object detection, object tracking, etc. Besides, the trustworthiness of deep learning models for medical applications is challenged, especially during the global outbreak of the COVID-19 pandemic in 2019.

This thesis addresses two critical questions: Firstly, can we develop practical adversarial attacks against deep learning applications? Secondly, how can we use interpretation methods to safely embrace deep learning models?

To answer these questions, a real-time white-box attack against the NVIDIA end-to-end driving model is presented. For modular autonomous driving systems, we further devise real-time white-box attacks against object detection models, and then propose the Human-in-the-Middle hardware attack to inject the Universal Adversarial Perturbation (UAP) into a USB camera. The effect of the attack against object tracking models is also studied. In addition, the distributed black-box attack is proposed to accelerate black-box attacks against machine-learning cloud services. Lastly, we use interpretation methods to understand how deep learning models make decisions and improve the trustworthiness of medical applications.

% Given that testing safety-critical edge cases in simulators is more efficient in autonomous driving. 

%Chapter \ref{chpt:patch} introduces physical patches in the Carla Simulator. 
